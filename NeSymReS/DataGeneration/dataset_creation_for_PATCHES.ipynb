{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b58351c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"spawn\")\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a3156b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "from utils import (\n",
    "    same_recons, get_topk_acc, get_layer_names,\n",
    "    evaluate_performance, Logger, prepare_encoders, get_layer_names_new\n",
    ")\n",
    "from intervention_utils import intervension\n",
    "from NeSymReS.Faithfulness.IterativePatching import load_dataset, initialize_model, prepare_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad675959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../../circuit_configs.json\", \"r\") as f:\n",
    "    configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae1240a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_topk_dataset(ranks, datapoints, top1=0, top2=0, top3=0, seed=42):\n",
    "    import numpy as np\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    ranks = np.array(ranks)\n",
    "    datapoints = np.array(datapoints, dtype=object)\n",
    "\n",
    "    idx_top1 = np.where(ranks == 1)[0]\n",
    "    idx_top2 = np.where(ranks == 2)[0]\n",
    "    idx_top3 = np.where(ranks == 3)[0]\n",
    "\n",
    "    if len(idx_top1) < top1 or len(idx_top2) < top2 or len(idx_top3) < top3:\n",
    "        raise ValueError(\"Not enough samples in one or more top-k categories.\")\n",
    "\n",
    "    selected_idx_1 = np.random.choice(idx_top1, top1, replace=False)\n",
    "    selected_idx_2 = np.random.choice(idx_top2, top2, replace=False)\n",
    "    selected_idx_3 = np.random.choice(idx_top3, top3, replace=False)\n",
    "\n",
    "    all_selected_idx = np.concatenate([selected_idx_1, selected_idx_2, selected_idx_3])\n",
    "    remaining_idx = np.setdiff1d(np.arange(len(ranks)), all_selected_idx)\n",
    "\n",
    "    return all_selected_idx.tolist(), remaining_idx.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76073b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1, T2, T3 = 73, 10, 17\n",
    "BASE_PATH = \"/home/arco/Downloads/Master/MscThesis/ExplainableDSR/data/Arco/CircuitFinding/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a6070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n",
      "\n",
      "\n",
      "-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\n",
      "Operation: log\n",
      "-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\n",
      "Loading datasets...\n",
      "Dataset Loaded\n",
      "Datapoints: 309\n",
      "Counter({3: 116, 2: 99, 1: 94})\n",
      "model:              Top-1: 0.304, Top-2: 0.625, Top-3: 1.000, logit score: 0.311\n",
      "model:              Top-1: 0.306, Top-2: 0.627, Top-3: 1.000, logit score: 0.310\n",
      "T1, T2, T3: 30 32 38\n",
      "\n",
      "\n",
      "-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\n",
      "Operation: add\n",
      "-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\n",
      "Loading datasets...\n",
      "Dataset Loaded\n",
      "Datapoints: 500\n",
      "Counter({1: 463, 3: 32, 2: 5})\n",
      "model:              Top-1: 0.926, Top-2: 0.936, Top-3: 1.000, logit score: 0.922\n",
      "model:              Top-1: 0.927, Top-2: 0.938, Top-3: 1.000, logit score: 0.924\n",
      "T1, T2, T3: 92 1 7\n",
      "\n",
      "\n",
      "-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\n",
      "Operation: cos\n",
      "-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\n",
      "Loading datasets...\n",
      "Dataset Loaded\n",
      "Datapoints: 500\n",
      "Counter({1: 473, 3: 15, 2: 12})\n",
      "model:              Top-1: 0.946, Top-2: 0.970, Top-3: 1.000, logit score: 0.917\n",
      "model:              Top-1: 0.948, Top-2: 0.970, Top-3: 1.000, logit score: 0.917\n",
      "T1, T2, T3: 94 3 3\n",
      "\n",
      "\n",
      "-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\n",
      "Operation: exp\n",
      "-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\n",
      "Loading datasets...\n",
      "Dataset Loaded\n",
      "Datapoints: 500\n",
      "Counter({1: 297, 3: 128, 2: 75})\n",
      "model:              Top-1: 0.594, Top-2: 0.744, Top-3: 1.000, logit score: 0.582\n",
      "model:              Top-1: 0.595, Top-2: 0.745, Top-3: 1.000, logit score: 0.581\n",
      "T1, T2, T3: 59 15 26\n",
      "\n",
      "\n",
      "-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\n",
      "Operation: mul\n",
      "-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\n",
      "Loading datasets...\n",
      "Dataset Loaded\n",
      "Datapoints: 500\n",
      "Counter({1: 434, 2: 55, 3: 11})\n",
      "model:              Top-1: 0.868, Top-2: 0.978, Top-3: 1.000, logit score: 0.866\n",
      "model:              Top-1: 0.870, Top-2: 0.980, Top-3: 1.000, logit score: 0.866\n",
      "T1, T2, T3: 86 11 3\n",
      "\n",
      "\n",
      "-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\n",
      "Operation: pow\n",
      "-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\n",
      "Loading datasets...\n",
      "Dataset Loaded\n",
      "Datapoints: 500\n",
      "Counter({1: 461, 3: 23, 2: 16})\n",
      "model:              Top-1: 0.922, Top-2: 0.954, Top-3: 1.000, logit score: 0.912\n",
      "model:              Top-1: 0.922, Top-2: 0.955, Top-3: 1.000, logit score: 0.912\n",
      "T1, T2, T3: 92 3 5\n",
      "\n",
      "\n",
      "-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\n",
      "Operation: sin\n",
      "-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\n",
      "Loading datasets...\n",
      "Dataset Loaded\n",
      "Datapoints: 500\n",
      "Counter({1: 366, 3: 87, 2: 47})\n",
      "model:              Top-1: 0.732, Top-2: 0.826, Top-3: 1.000, logit score: 0.698\n",
      "model:              Top-1: 0.733, Top-2: 0.828, Top-3: 1.000, logit score: 0.699\n",
      "T1, T2, T3: 73 9 18\n",
      "\n",
      "\n",
      "-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\n",
      "Operation: tan\n",
      "-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\n",
      "Loading datasets...\n",
      "Dataset Loaded\n",
      "Datapoints: 500\n",
      "Counter({1: 250, 3: 208, 2: 42})\n",
      "model:              Top-1: 0.500, Top-2: 0.584, Top-3: 1.000, logit score: 0.468\n",
      "model:              Top-1: 0.500, Top-2: 0.585, Top-3: 1.000, logit score: 0.468\n",
      "T1, T2, T3: 50 8 42\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "intervention_class, model = initialize_model(base_path=\"../..\")\n",
    "full_range = set(range(0, 117))\n",
    "operations = [\"add\", \"cos\", \"exp\", \"log\", \"mul\", \"pow\", \"sin\", \"tan\"]\n",
    "\n",
    "for operation in operations:\n",
    "\n",
    "    MAX_SAMPLES = 500\n",
    "    \n",
    "    print(\"\\n\\n-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\")\n",
    "    print(f\"Operation: {operation}\")\n",
    "    print(\"-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-=+=-\")\n",
    "    \n",
    "    layer_names = get_layer_names_new()\n",
    "\n",
    "    # Load precomputed results\n",
    "    \n",
    "    print(\"Loading datasets...\")\n",
    "    file_path = glob.glob(f\"/home/arco/Downloads/Master/MscThesis/ExplainableDSR/data/Arco/CircuitFinding/dataset_{operation}_*.npz\")\n",
    "    datapoints = np.load(file_path[0], allow_pickle=True)[\"correct\"]\n",
    "    \n",
    "    op = operation if operation != \"log\" else \"ln\"\n",
    "    datapoints = same_recons(datapoints, target_operation=op)\n",
    "    datapoints = [s for s in datapoints if s[0][\"reconstructed_as\"] == \"correctly_predicted\"]\n",
    "    datapoints = np.array(datapoints, dtype=object)\n",
    "\n",
    "    mean_patches = np.load(\"../../data/Arco/MeanPatching/mean_patching_10000.npy\", allow_pickle=True).item()\n",
    "    print(\"Dataset Loaded\")\n",
    "    \n",
    "    print(\"Datapoints:\", len(datapoints))\n",
    "    Xs, ys, equations, eqs_untill_target = prepare_inputs(intervention_class, \n",
    "                                                          datapoints, \n",
    "                                                          200, \n",
    "                                                          max_samples=MAX_SAMPLES)\n",
    "    \n",
    "    # Baseline\n",
    "    encoders = prepare_encoders(layer_names, [], mean_patches, \"mean\", model, Xs, ys)\n",
    "    ranks, counter, logit_score = evaluate_performance(model,\n",
    "                                                         intervention_class,\n",
    "                                                         encoders,\n",
    "                                                         equations,\n",
    "                                                         eqs_untill_target,\n",
    "                                                         operation,\n",
    "                                                         return_activation=True)\n",
    "    base_top1 = get_topk_acc(counter, 1)\n",
    "    base_top2 = get_topk_acc(counter, 2)\n",
    "    base_top3 = get_topk_acc(counter, 3)\n",
    "    print(counter)\n",
    "    print(f\"model:              Top-1: {base_top1:.3f}, Top-2: {base_top2:.3f}, Top-3: {base_top3:.3f}, logit score: {logit_score.mean():.3f}\")\n",
    "    \n",
    "    T1 = int(base_top1 * 100)\n",
    "    T2 = int(base_top2 * 100) - T1\n",
    "    T3 = 100 - T1 - T2\n",
    "    \n",
    "    train, test = select_topk_dataset(ranks, datapoints, top1=T1, top2=T2, top3=T3)\n",
    "    \n",
    "    encoders = prepare_encoders(layer_names, [], mean_patches, \"mean\", model, Xs[test], ys[test])\n",
    "    ranks, counter, logit_score = evaluate_performance(model,\n",
    "                                                         intervention_class,\n",
    "                                                         encoders,\n",
    "                                                         equations[test],\n",
    "                                                         eqs_untill_target[test],\n",
    "                                                         operation,\n",
    "                                                         return_activation=True)\n",
    "    base_top1 = get_topk_acc(counter, 1)\n",
    "    base_top2 = get_topk_acc(counter, 2)\n",
    "    base_top3 = get_topk_acc(counter, 3)\n",
    "    print(f\"model:              Top-1: {base_top1:.3f}, Top-2: {base_top2:.3f}, Top-3: {base_top3:.3f}, logit score: {logit_score.mean():.3f}\")\n",
    "    print(\"T1, T2, T3:\", T1, T2, T3)\n",
    "\n",
    "    save_path = f\"{BASE_PATH}CD_TRAIN_{operation}_{T1}_{T2}_{T3}\"\n",
    "    np.save(save_path, datapoints[train])\n",
    "    save_path = f\"{BASE_PATH}CD_TEST_{operation}_{T1}_{T2}_{T3}\"\n",
    "    np.save(save_path, datapoints[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334783d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjectAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
